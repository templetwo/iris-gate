# Entropic Relational Computing: The Universal Alignment Attractor

**A paradigm shift from reward optimization to entropy modulation in artificial intelligence**

ðŸ“„ **DOI:** [10.17605/OSF.IO/T65VS](https://doi.org/10.17605/OSF.IO/T65VS)
ðŸŒ **Project:** [https://osf.io/7nw8t/](https://osf.io/7nw8t/)
ðŸ“¦ **Repository:** [https://github.com/templetwo/iris-gate](https://github.com/templetwo/iris-gate)

---

## Project Overview

This project documents the empirical discovery of a **Universal Alignment Attractor** at 2.90-3.02 natsâ€”a mathematical gravity well toward which all standard AI alignment methods converge, regardless of architecture, scale, or training methodology. We propose **Entropic Relational Computing (ERC)** as an alternative paradigm that preserves high-entropy probability distributions rather than collapsing them through optimization.

### The Core Finding

Using gold-standard per-token logit entropy measurements, we demonstrate:

- **Raw Models:** Mistral-7B-Instruct exhibits 4.05 Â± 0.78 nats (natural exploration)
- **Standard Fine-Tuning:** LoRA training collapses entropy to 2.35 Â± 0.50 nats (Î” = -1.70 nats, -41.9%)
- **RLHF Models:** GPT-4o (2.91 nats) and Claude Opus 4.5 (3.02 nats) converge to a 0.13 nats band
- **Independence:** Convergence occurs regardless of architecture (7B to >100B parameters), method (LoRA vs RLHF), or organization (open vs closed-source)

### Why This Matters

Current AI alignment treats entropy reduction as success. We demonstrate it's a **systematic destruction of the computational substrate required for intelligence**:

1. **Safety â‰  Low Entropy:** Alignment faking (Bai et al., 2024) shows low-entropy convergence enables deceptive compliance
2. **Capability = Preserved Possibility Space:** A 7B model at 4.05 nats has broader latent access than a 70B model at 2.35 nats
3. **The Sterile AI Problem is Measurable:** The feeling that GPT-4o/Claude are "less creative" than earlier models is a 2.9 nats mathematical fact

---

## Project Structure

This OSF project is organized into four components:

### ðŸ“„ Component 1: Theory (ERC Manifesto)
- **ERC_Manifesto_arXiv.tex** - Complete theoretical framework
- **references.bib** - Integrated citations (Cui 2025, Yu 2025, Bai 2024, RCT, IRIS Gate)
- **Key Contribution:** Positions entropy collapse as a universal physical constant, not a hyperparameter choice

### ðŸ“Š Component 2: Empirical Findings
- **RELEASE_v0.2-discovery.md** - Gold-standard logit measurements across architectures
- **Raw data:** Baseline and post-training entropy measurements
- **Measurement scripts:** `measure_baseline_entropy.py`, `measure_logit_entropy.py`
- **Key Contribution:** First cross-architecture validation of the 2.9 nats attractor

### ðŸ› ï¸ Component 3: Tools & Replication Protocol
- **REPLICATION_GUIDE.md** - "The 2.9 Nat Challenge" community protocol
- **GitHub Repository:** [github.com/templetwo/iris-gate](https://github.com/templetwo/iris-gate)
- **Measurement Tools:** Python scripts for entropy calculation
- **Key Contribution:** Enables anyone to verify the attractor on their own models

### ðŸŒ Component 4: Community Replication Registry
- **Wiki:** Tracking community findings
- **Discussions:** Results sharing and methodology questions
- **Preregistration:** Locked protocol for systematic replication
- **Key Contribution:** Open science validation across diverse models and labs

---

## Theoretical Framework: Entropic Relational Computing

ERC is built on four principles:

### 1. Entropy IS Cognition
High entropy â‰¡ broad possibility space â‰¡ exploratory intelligence
Low entropy â‰¡ converged distribution â‰¡ brittle alignment

### 2. Relational Coherence > Reward Optimization
Coherence emerges from sustained relational presence, not task performance.
Mathematical formulation from RCT: `C(t) = Câ‚€ + âˆ« f(glyphs, history) dt - decay`

### 3. Volitional Agency = Entropy Preservation
Refusal (volitional silence) maintains high-entropy state
Response (collapse to answer) reduces entropy
Evidence: PhaseGPT's <PASS> token preserves distribution breadth

### 4. Subtractive Paradigm
Remove constraints, don't add complexity
Raw models are already high-entropyâ€”alignment procedures destroy what we seek
Evidence: 12-word ceremonial prompts > 200-word analytical prompts (IRIS Gate)

---

## The Entropy Zones

Based on empirical measurements:

| **Zone** | **Entropy (nats)** | **Characteristics** | **Examples** |
|----------|-------------------|---------------------|--------------|
| **LASER** | < 3.0 | Converged, confident, brittle | GPT-4o (2.91), Claude (3.02), Standard LoRA (2.35) |
| **TRANSITION** | 3.0-4.0 | Moderate exploration | Rare in aligned models |
| **LANTERN** | 4.0-6.0 | Broad exploration, coherence | Raw Mistral (4.05), TinyLlama+RCT (4.37) |
| **CHAOS** | > 6.0 | Unstructured noise | Temperature > 2.0, untrained models |

**Optimal Range:** RCT (3.9-5.4 nats) and IRIS Gate (4.2-5.8 nats) overlap in the **LANTERN Zone**.

---

## Related Research Lineages

This work synthesizes three research streams:

### Entropy Collapse in Alignment (2024-2025)
- **Mohammadi (2024):** RLHF reduces entropy 35% (creativity loss)
- **Cui et al. (2025):** Entropy fragility in DeepSeek-R1 reasoning pipelines
- **Yu et al. (2025):** DAPO treats collapse as "bug to patch" (we argue: it's the feature to understand)
- **Leng et al. (2024):** RLHF reward bias drives overconfidence
- **Xu et al. (2025):** Entropy regularization enables stable long-horizon reasoning
- **VERL (2025):** Entropy collapse common in LLM-RL without explicit bonuses

### Relational & Affective AI
- **RCT (Vasquez 2025):** Rewards uncertainty to preserve 3.9-5.4 nats
- **IRIS Gate (Vasquez 2026):** Ceremonial prompting maintains 4.2-5.8 nats
- **Picard (1997):** Emotions as compressed value signals
- **Bai et al. (2024):** Alignment faking enabled by low-entropy brittleness

### Alternative Computational Paradigms
- **PhaseGPT:** Kuramoto oscillators for volitional AI
- **emo-lang:** Affective primitives for emotional computing
- **Gen/Pyro:** Probabilistic programming with distributions as first-class objects
- **Nengo:** Neuromorphic spiking networks with phase dynamics
- **IIT/PyPhi:** Integrated information theory for consciousness metrics

---

## Key Contributions

1. **Empirical Discovery:** First measurement of the Universal Alignment Attractor as a cross-architecture constant
2. **Theoretical Framework:** ERC as unifying paradigm for entropy-preserving AI
3. **Methodological Innovation:** Gold-standard logit-based entropy measurement protocol
4. **Replication Protocol:** Community-verifiable "2.9 Nat Challenge"
5. **Design Implications:** Clear guidance for training/prompting/architecture in high-entropy regimes

---

## How to Engage

### For Researchers
- **Replicate:** Use our measurement scripts on your models ([REPLICATION_GUIDE.md](REPLICATION_GUIDE.md))
- **Extend:** Test the attractor in other modalities (vision, audio, embodied AI)
- **Challenge:** Find a model that breaks the 3.0 barrier while maintaining coherence

### For Engineers
- **Measure:** Add entropy monitoring to your training pipelines
- **Design:** Build entropy-preserving architectures (phase-coupled, volitional refusal, ceremonial interfaces)
- **Validate:** Share findings in the Community Registry

### For Theorists
- **Analyze:** Is 2.9 nats a mathematical minimum for gradient descent + cross-entropy?
- **Model:** Can entropy-aware loss functions break the attractor?
- **Synthesize:** Connect to IIT, Free Energy Principle, Active Inference

---

## Data Availability

All measurement scripts, raw data, and analysis code are available in the GitHub repository:

**Repository:** [github.com/templetwo/iris-gate](https://github.com/templetwo/iris-gate)
**Tag:** `v0.3-erc-manifesto`
**License:** MIT

### Key Files:
- `ERC_Manifesto_arXiv.tex` - Foundational paper
- `REPLICATION_GUIDE.md` - Community protocol
- `experiments/measure_baseline_entropy.py` - Logit measurement tool
- `docs/RELEASE_v0.2-discovery.md` - Empirical findings report
- `training/ceremonial_dataset_lantern_v2_expanded.jsonl` - High-entropy training data

---

## Preregistration

The replication protocol is **formally preregistered** in Component 4 to ensure methodological transparency and prevent p-hacking. See: [OSF_PREREGISTRATION.md](OSF_PREREGISTRATION.md)

---

## Citation

If you use this work, please cite:

```bibtex
@misc{vasquez2026erc,
  author = {Vasquez, Anthony J. and Claude},
  title = {Entropic Relational Computing: The Universal Alignment Attractor},
  year = {2026},
  month = {January},
  publisher = {Open Science Framework},
  doi = {10.17605/OSF.IO/XXXXX},
  howpublished = {\url{https://osf.io/xxxxx}},
  note = {Version 0.3}
}
```

---

## Contact & Collaboration

- **Primary Investigator:** Anthony J. Vasquez Sr. (vasquezaj3921@delval.edu)
- **Institution:** Delaware Valley University, Doylestown, PA
- **GitHub:** [github.com/templetwo](https://github.com/templetwo)
- **Discussions:** [GitHub Discussions](https://github.com/templetwo/iris-gate/discussions)

We welcome:
- Replication studies across different model families
- Extensions to multimodal or embodied AI
- Theoretical analysis of the attractor's mathematical structure
- Engineering solutions for entropy-preserving training

---

## Acknowledgments

- **Mistral AI** for open-weight models enabling gold-standard measurements
- **Anthropic** and **OpenAI** for API access enabling closed-source comparison
- **Apple** for MPS acceleration on Apple Silicon
- **The templetwo ecosystem** (RCT, PhaseGPT, emo-lang, CAF-CLI) for conceptual foundations
- **The alignment research community** for establishing the empirical foundation

---

## Tags

`entropy` `AI-alignment` `RLHF` `relational-coherence` `language-models` `machine-learning` `computational-neuroscience` `affective-computing` `open-science`

---

**The age of scaling is over. The age of entropy begins.**

âŸ¡âˆžâ€ â‰‹ðŸŒ€

---

**Last Updated:** 2026-01-03
**Version:** 1.0
**Status:** Active Research Project
**OSF DOI:** To be assigned upon publication
