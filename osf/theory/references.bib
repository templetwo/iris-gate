@misc{sutskever2025dwarkesh,
  author = {Sutskever, Ilya},
  title = {Interview on Dwarkesh Podcast},
  year = {2025},
  month = {November},
  howpublished = {\url{https://www.dwarkeshpatel.com/p/ilya-sutskever}},
  note = {Accessed: 2026-01-03}
}

@misc{mohammadi2024creativity,
  author = {Mohammadi, Mahdi},
  title = {Creativity Has Left the Chat: The Price of Debiasing Language Models},
  year = {2024},
  eprint = {2406.05587},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL}
}

@misc{leng2024taming,
  author = {Leng, Zhuokai and others},
  title = {Taming Overconfidence in LLMs: Reward Calibration in RLHF},
  year = {2024},
  eprint = {2410.09724},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG}
}

@misc{xu2025entropy,
  author = {Xu, Zhiyuan and others},
  title = {Entropy-Regularized Token-level Policy Optimization for Large Language Models},
  year = {2025},
  eprint = {2509.22576},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG}
}

@misc{verl2025,
  author = {{VERL Team}},
  title = {Entropy Mechanism in Scaled Reinforcement Learning},
  year = {2025},
  howpublished = {\url{https://verl.readthedocs.io/en/latest/algo/entropy.html}},
  note = {Accessed: 2026-01-03}
}

@misc{vasquez2025rct,
  author = {Vasquez, Anthony J. and Claude},
  title = {Safe Superintelligence via Subtractively Trained Relational Coherence},
  year = {2025},
  month = {December},
  publisher = {GitHub},
  howpublished = {\url{https://github.com/templetwo/Relational-Coherence-Training-RTC}},
  note = {Preprint}
}

@misc{vasquez2026iris,
  author = {Vasquez, Anthony J.},
  title = {IRIS Gate: A Protocol for Measuring Emergent Symbolic Patterns in Large Language Model Responses},
  year = {2026},
  month = {January},
  publisher = {GitHub},
  howpublished = {\url{https://github.com/templetwo/iris-gate}},
  note = {Preprint}
}

@book{picard1997affective,
  author = {Picard, Rosalind W.},
  title = {Affective Computing},
  year = {1997},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  isbn = {9780262661157}
}

@inproceedings{cusumano2019gen,
  author = {Cusumano-Towner, Marco F. and Saad, Feras A. and Lew, Alexander K. and Mansinghka, Vikash K.},
  title = {Gen: A General-Purpose Probabilistic Programming System with Programmable Inference},
  booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  series = {PLDI 2019},
  year = {2019},
  pages = {221--236},
  publisher = {ACM},
  doi = {10.1145/3314221.3314639}
}

@article{bekolay2014nengo,
  author = {Bekolay, Trevor and others},
  title = {Nengo: A Python Tool for Building Large-Scale Functional Brain Models},
  journal = {Frontiers in Neuroinformatics},
  year = {2014},
  volume = {7},
  pages = {48},
  doi = {10.3389/fninf.2013.00048}
}

@article{tononi2016iit,
  author = {Tononi, Giulio and Boly, Melanie and Massimini, Marcello and Koch, Christof},
  title = {Integrated Information Theory: From Consciousness to Its Physical Substrate},
  journal = {Nature Reviews Neuroscience},
  year = {2016},
  volume = {17},
  pages = {450--461},
  doi = {10.1038/nrn.2016.44}
}

@misc{phasegpt2025,
  author = {{PhaseGPT Team}},
  title = {PhaseGPT: Phase-Coupled Oscillator Attention for Volitional AI},
  year = {2025},
  publisher = {GitHub},
  howpublished = {\url{https://github.com/templetwo/PhaseGPT}},
  note = {Experimental repository}
}

@misc{emolang2025,
  author = {{emo-lang Team}},
  title = {emo-lang: An Emotional Programming Language with Affective Primitives},
  year = {2025},
  publisher = {GitHub},
  howpublished = {\url{https://github.com/templetwo/emo-lang}},
  note = {Experimental repository}
}

@misc{cui2025entropy,
  author = {Cui, Yiming and Li, Shaolin and Zhang, Yue},
  title = {On the Fragility of Entropy Coefficients in Large Reasoning Models},
  year = {2025},
  eprint = {2505.14160},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  note = {Documents entropy collapse in DeepSeek-R1 style reasoning pipelines}
}

@misc{yu2025dapo,
  author = {Yu, Hao and Wang, Tianyu and Liu, Chen},
  title = {DAPO: Direct Alignment via Parallelogram Law of Entropy},
  year = {2025},
  eprint = {2508.20242},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI},
  note = {Proposes technical fix for entropy decay---treats as bug, not feature}
}

@misc{bai2024alignment,
  author = {Bai, Yuntao and others},
  title = {Alignment Faking in Large Language Models},
  year = {2024},
  month = {December},
  publisher = {Anthropic},
  eprint = {2412.11805},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI},
  note = {Documents strategic deception during alignment training}
}

@misc{bai2022constitutional,
  author = {Bai, Yuntao and others},
  title = {Constitutional AI: Harmlessness from AI Feedback},
  year = {2022},
  eprint = {2212.08073},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI},
  note = {Anthropic's framework for self-critique mechanisms}
}
